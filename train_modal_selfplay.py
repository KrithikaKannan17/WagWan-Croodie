"""
Modal training script for self-play reinforcement learning.
Trains the chess model on self-play data generated by MCTS.
Updated: 2025-11-16 - Fixed data format compatibility
"""

import modal
import sys

# Persistent volume for datasets and models
volume = modal.Volume.from_name("selfplay-vol", create_if_missing=True)

# Image with dependencies
image = (
    modal.Image.debian_slim(python_version="3.10")
    .pip_install("numpy==1.26.4")
    .pip_install("torch==2.2.2", "python-chess")
    .add_local_dir(".", remote_path="/root")
)

app = modal.App("selfplay-training")


@app.function(
    image=image,
    gpu="A10G",
    timeout=7200,  # 2 hours
    volumes={"/data": volume},
)
def train_selfplay(
    data_file: str = "datasets/test_10g.npz",
    model_input: str = "chess_model_improved.pth",
    model_output: str = "chess_model_selfplay.pth",
    epochs: int = 10,
    batch_size: int = 256,
    learning_rate: float = 0.001,
):
    import os
    import sys
    import numpy as np
    import torch
    import torch.nn as nn
    import torch.optim as optim
    from torch.utils.data import Dataset, DataLoader
    
    os.chdir("/root")
    sys.path.insert(0, "/root")
    
    from src.utils.model import ChessModel
    
    print("="*70)
    print("ðŸš€ Self-Play Reinforcement Learning Training")
    print("="*70)
    print(f"Data: /data/{data_file}")
    print(f"Input model: /root/{model_input}")
    print(f"Output model: /data/{model_output}")
    print(f"Epochs: {epochs}, Batch size: {batch_size}, LR: {learning_rate}")
    print("="*70)
    
    # Load data
    data_path = f"/data/{data_file}"
    if not os.path.exists(data_path):
        raise FileNotFoundError(f"Data file not found: {data_path}")
    
    data = np.load(data_path)
    boards = torch.from_numpy(data['boards']).float()

    # Handle different key names (policies/policy_targets, values/value_targets)
    if 'policy_targets' in data:
        policy_targets = torch.from_numpy(data['policy_targets']).float()
        value_targets = torch.from_numpy(data['value_targets']).float()
    else:
        policy_targets = torch.from_numpy(data['policies']).float()
        value_targets = torch.from_numpy(data['values']).float()

    print(f"\nâœ“ Loaded {len(boards)} positions")
    print(f"  Boards: {boards.shape}")
    print(f"  Policies: {policy_targets.shape}")
    print(f"  Values: {value_targets.shape}")
    
    # Create dataset
    class SelfPlayDataset(Dataset):
        def __init__(self, boards, policies, values):
            self.boards = boards
            self.policies = policies
            self.values = values
        
        def __len__(self):
            return len(self.boards)
        
        def __getitem__(self, idx):
            return self.boards[idx], self.policies[idx], self.values[idx]
    
    dataset = SelfPlayDataset(boards, policy_targets, value_targets)
    
    # Split into train/val (90/10)
    train_size = int(0.9 * len(dataset))
    val_size = len(dataset) - train_size
    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])
    
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
    
    print(f"\nâœ“ Train: {train_size}, Val: {val_size}")
    
    # Load model
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = ChessModel(num_residual_blocks=6, channels=64)

    model_path = f"/root/{model_input}"
    if os.path.exists(model_path):
        checkpoint = torch.load(model_path, map_location=device)

        # Handle different checkpoint formats
        if isinstance(checkpoint, dict) and "model_state_dict" in checkpoint:
            # Checkpoint with metadata (from train_improved.py)
            model.load_state_dict(checkpoint["model_state_dict"])
            print(f"\nâœ“ Loaded model from {model_path} (checkpoint format)")
        else:
            # Direct state_dict
            model.load_state_dict(checkpoint)
            print(f"\nâœ“ Loaded model from {model_path} (state_dict format)")
    else:
        print(f"\nâš  Model not found, training from scratch")
    
    model = model.to(device)
    
    # Optimizer and loss
    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)
    policy_criterion = nn.CrossEntropyLoss()
    value_criterion = nn.MSELoss()
    
    # Training loop
    print(f"\n{'='*70}")
    print("Training...")
    print(f"{'='*70}\n")
    
    best_val_loss = float('inf')
    
    for epoch in range(epochs):
        # Train
        model.train()
        train_policy_loss = 0
        train_value_loss = 0
        
        for boards_batch, policies_batch, values_batch in train_loader:
            boards_batch = boards_batch.to(device)
            policies_batch = policies_batch.to(device)
            values_batch = values_batch.to(device)
            
            optimizer.zero_grad()

            # Forward pass
            policy_logits, value_pred = model(boards_batch)

            # Compute losses
            policy_loss = policy_criterion(policy_logits, policies_batch)
            value_loss = value_criterion(value_pred.squeeze(), values_batch)
            total_loss = policy_loss + value_loss

            # Backward pass
            total_loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()

            train_policy_loss += policy_loss.item()
            train_value_loss += value_loss.item()

        train_policy_loss /= len(train_loader)
        train_value_loss /= len(train_loader)

        # Validation
        model.eval()
        val_policy_loss = 0
        val_value_loss = 0

        with torch.no_grad():
            for boards_batch, policies_batch, values_batch in val_loader:
                boards_batch = boards_batch.to(device)
                policies_batch = policies_batch.to(device)
                values_batch = values_batch.to(device)

                policy_logits, value_pred = model(boards_batch)

                policy_loss = policy_criterion(policy_logits, policies_batch)
                value_loss = value_criterion(value_pred.squeeze(), values_batch)

                val_policy_loss += policy_loss.item()
                val_value_loss += value_loss.item()

        val_policy_loss /= len(val_loader)
        val_value_loss /= len(val_loader)
        val_total_loss = val_policy_loss + val_value_loss

        print(f"Epoch {epoch+1}/{epochs} | "
              f"Train P: {train_policy_loss:.4f} V: {train_value_loss:.4f} | "
              f"Val P: {val_policy_loss:.4f} V: {val_value_loss:.4f} Total: {val_total_loss:.4f}")

        # Save best model
        if val_total_loss < best_val_loss:
            best_val_loss = val_total_loss
            output_path = f"/data/{model_output}"
            torch.save(model.state_dict(), output_path)
            print(f"  âœ“ Saved best model (val_loss: {val_total_loss:.4f})")

    volume.commit()

    print(f"\n{'='*70}")
    print(f"âœ… Training complete!")
    print(f"  Best val loss: {best_val_loss:.4f}")
    print(f"  Model saved: /data/{model_output}")
    print(f"{'='*70}")

    return output_path


@app.function(image=image, volumes={"/data": volume})
def download_model(filename: str):
    import os
    path = "/data/" + filename
    if not os.path.exists(path):
        raise FileNotFoundError(path)
    return open(path, "rb").read()


@app.local_entrypoint()
def main(
    data_file: str = "datasets/test_10g.npz",
    model_input: str = "chess_model_improved.pth",
    model_output: str = "chess_model_selfplay.pth",
    epochs: int = 10,
    batch_size: int = 256,
    learning_rate: float = 0.001,
    download: bool = False,
):
    if download:
        data = download_model.remote(model_output)
        with open(model_output, "wb") as f:
            f.write(data)
        print(f"Downloaded: {model_output}")
        return

    print("â–¶ Training on Modal GPUâ€¦")
    path = train_selfplay.remote(
        data_file=data_file,
        model_input=model_input,
        model_output=model_output,
        epochs=epochs,
        batch_size=batch_size,
        learning_rate=learning_rate,
    )
    print(f"Model saved: {path}")


