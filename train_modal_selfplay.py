"""
Modal training script for self-play reinforcement learning.
Trains the chess model on self-play data generated by MCTS.
Updated: 2025-11-16 - Fixed data format compatibility
"""

import modal
import sys

# Persistent volume for datasets and models
volume = modal.Volume.from_name("selfplay-vol", create_if_missing=True)

# Image with dependencies
image = (
    modal.Image.debian_slim(python_version="3.10")
    .pip_install("numpy==1.26.4")
    .pip_install("torch==2.2.2", "python-chess")
    .add_local_dir(".", remote_path="/root")
)

app = modal.App("selfplay-training")


@app.function(
    image=image,
    gpu="A10G",
    timeout=7200,  # 2 hours
    volumes={"/data": volume},
)
def train_selfplay(
    data_file: str = "selfplay_phase2.npz",
    model_input: str = "chess_model_best.pth",
    model_output: str = "chess_model_sp_v1.pth",
    epochs: int = 5,
    batch_size: int = 256,
    learning_rate: float = 0.001,
):
    import os
    import sys
    import numpy as np
    import torch
    import torch.nn as nn
    import torch.optim as optim
    from torch.utils.data import Dataset, DataLoader
    
    os.chdir("/root")
    sys.path.insert(0, "/root")
    
    from src.utils.model import ChessModel
    
    print("="*70)
    print("üöÄ PHASE 2: SELF-PLAY TRAINING ON MODAL GPU")
    print("="*70)
    print(f"Data: /data/{data_file}")
    print(f"Input model: /root/{model_input}")
    print(f"Output model: /data/data/{model_output}")
    print(f"Epochs: {epochs}, Batch size: {batch_size}, LR: {learning_rate}")
    print("="*70)
    
    # Load data
    data_path = f"/data/data/{data_file}"
    if not os.path.exists(data_path):
        # Try without extra /data/ for backward compatibility
        data_path = f"/data/{data_file}"
        if not os.path.exists(data_path):
            raise FileNotFoundError(f"Data file not found in /data/data/ or /data/")
    
    data = np.load(data_path)
    boards = torch.from_numpy(data['boards']).float()

    # Handle different key names (policies/policy_targets, values/value_targets)
    if 'policy_targets' in data:
        policy_targets = torch.from_numpy(data['policy_targets']).float()
        value_targets = torch.from_numpy(data['value_targets']).float()
    else:
        policy_targets = torch.from_numpy(data['policies']).float()
        value_targets = torch.from_numpy(data['values']).float()

    print(f"\n‚úì Loaded {len(boards)} positions")
    print(f"  Boards: {boards.shape}")
    print(f"  Policies: {policy_targets.shape}")
    print(f"  Values: {value_targets.shape}")
    
    # Create dataset
    class SelfPlayDataset(Dataset):
        def __init__(self, boards, policies, values):
            self.boards = boards
            self.policies = policies
            self.values = values
        
        def __len__(self):
            return len(self.boards)
        
        def __getitem__(self, idx):
            return self.boards[idx], self.policies[idx], self.values[idx]
    
    dataset = SelfPlayDataset(boards, policy_targets, value_targets)
    
    # Split into train/val (90/10)
    train_size = int(0.9 * len(dataset))
    val_size = len(dataset) - train_size
    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])
    
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
    
    print(f"\n‚úì Train: {train_size}, Val: {val_size}")
    
    # Load model
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    model_path = f"/root/{model_input}"
    if os.path.exists(model_path):
        checkpoint = torch.load(model_path, map_location=device, weights_only=False)

        # Get state_dict
        if isinstance(checkpoint, dict) and "model_state_dict" in checkpoint:
            state_dict = checkpoint["model_state_dict"]
        else:
            state_dict = checkpoint
            checkpoint = {}
        
        # Detect architecture from state_dict shapes
        if 'initial_conv.weight' in state_dict:
            channels = state_dict['initial_conv.weight'].shape[0]
            num_residual_blocks = sum(1 for key in state_dict.keys() if key.startswith('residual_blocks.') and key.endswith('.conv1.weight'))
            print(f"\n‚úì Detected architecture: {num_residual_blocks} blocks, {channels} channels")
        else:
            num_residual_blocks = checkpoint.get('num_residual_blocks', 6)
            channels = checkpoint.get('channels', 64)
            print(f"\n‚úì Using default architecture: {num_residual_blocks} blocks, {channels} channels")
        
        # Create model with detected architecture
        model = ChessModel(num_residual_blocks=num_residual_blocks, channels=channels)
        
        # Filter out incompatible keys (different shapes)
        model_state = model.state_dict()
        filtered_state_dict = {}
        skipped_keys = []
        
        for key, value in state_dict.items():
            if key in model_state:
                if model_state[key].shape == value.shape:
                    filtered_state_dict[key] = value
                else:
                    skipped_keys.append(key)
            else:
                skipped_keys.append(key)
        
        # Load filtered state_dict
        incompatible_keys = model.load_state_dict(filtered_state_dict, strict=False)
        
        if skipped_keys or incompatible_keys.missing_keys:
            print(f"  ‚ö†Ô∏è  Partial load: Skipped {len(skipped_keys)} incompatible keys")
            print(f"     Body (residual blocks) loaded, heads will be trained from scratch")
        
        print(f"‚úì Loaded model from {model_path}")
    else:
        print(f"\n‚ö† Model not found, training from scratch")
        model = ChessModel(num_residual_blocks=6, channels=64)
    
    model = model.to(device)
    
    # Optimizer and loss
    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)
    policy_criterion = nn.CrossEntropyLoss()
    value_criterion = nn.MSELoss()
    
    # Training loop
    print(f"\n{'='*70}")
    print("Training...")
    print(f"{'='*70}\n")
    
    best_val_loss = float('inf')
    
    for epoch in range(epochs):
        # Train
        model.train()
        train_policy_loss = 0
        train_value_loss = 0
        
        for boards_batch, policies_batch, values_batch in train_loader:
            boards_batch = boards_batch.to(device)
            policies_batch = policies_batch.to(device)
            values_batch = values_batch.to(device)
            
            optimizer.zero_grad()

            # Forward pass
            policy_logits, value_pred = model(boards_batch)

            # Compute losses
            policy_loss = policy_criterion(policy_logits, policies_batch)
            value_loss = value_criterion(value_pred.squeeze(), values_batch)
            total_loss = policy_loss + value_loss

            # Backward pass
            total_loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()

            train_policy_loss += policy_loss.item()
            train_value_loss += value_loss.item()

        train_policy_loss /= len(train_loader)
        train_value_loss /= len(train_loader)

        # Validation
        model.eval()
        val_policy_loss = 0
        val_value_loss = 0

        with torch.no_grad():
            for boards_batch, policies_batch, values_batch in val_loader:
                boards_batch = boards_batch.to(device)
                policies_batch = policies_batch.to(device)
                values_batch = values_batch.to(device)

                policy_logits, value_pred = model(boards_batch)

                policy_loss = policy_criterion(policy_logits, policies_batch)
                value_loss = value_criterion(value_pred.squeeze(), values_batch)

                val_policy_loss += policy_loss.item()
                val_value_loss += value_loss.item()

        val_policy_loss /= len(val_loader)
        val_value_loss /= len(val_loader)
        val_total_loss = val_policy_loss + val_value_loss

        print(f"Epoch {epoch+1}/{epochs} | "
              f"Train P: {train_policy_loss:.4f} V: {train_value_loss:.4f} | "
              f"Val P: {val_policy_loss:.4f} V: {val_value_loss:.4f} Total: {val_total_loss:.4f}")

        # Save best model
        if val_total_loss < best_val_loss:
            best_val_loss = val_total_loss
            output_path = f"/data/{model_output}"
            torch.save(model.state_dict(), output_path)
            print(f"  ‚úì Saved best model (val_loss: {val_total_loss:.4f})")

    volume.commit()

    print(f"\n{'='*70}")
    print(f"‚úÖ Training complete!")
    print(f"  Best val loss: {best_val_loss:.4f}")
    print(f"  Model saved: /data/{model_output}")
    print(f"{'='*70}")

    return output_path


@app.function(image=image, volumes={"/data": volume})
def download_model(filename: str):
    import os
    path = "/data/" + filename
    if not os.path.exists(path):
        raise FileNotFoundError(path)
    return open(path, "rb").read()


@app.local_entrypoint()
def main(
    data_file: str = "selfplay_phase2.npz",
    model_input: str = "chess_model_best.pth",
    model_output: str = "chess_model_sp_v1.pth",
    epochs: int = 5,
    batch_size: int = 256,
    learning_rate: float = 0.001,
    download: bool = False,
):
    if download:
        data = download_model.remote(model_output)
        with open(model_output, "wb") as f:
            f.write(data)
        print(f"Downloaded: {model_output}")
        return

    print("‚ñ∂ Training on Modal GPU‚Ä¶")
    path = train_selfplay.remote(
        data_file=data_file,
        model_input=model_input,
        model_output=model_output,
        epochs=epochs,
        batch_size=batch_size,
        learning_rate=learning_rate,
    )
    print(f"Model saved: {path}")


